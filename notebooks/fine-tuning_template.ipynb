{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "#GPUの指定\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "#pre_train_modelの指定\n",
    "MODEL_NAME='cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "#batch_size\n",
    "batch=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"tmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-val-testを8:1:1で分ける本当は固めといた方が良い\n",
    "index_train=int(len(df)*0.8)\n",
    "index_val=int(len(df)*0.1)\n",
    "index_test=int(len(df)*0.1)\n",
    "df_train=df.iloc[0:index_train,1:]\n",
    "df_val=df.iloc[index_train:index_train+index_val,1:]\n",
    "df_test=df.iloc[index_train+index_val:index_train+index_val+index_test,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "L=LabelEncoder()\n",
    "df[\"target_label\"]=L.fit_transform(df[\"target_label\"])\n",
    "df_train[\"target_label\"]=L.transform(df_train[\"target_label\"])\n",
    "df_test[\"target_label\"]=L.transform(df_test[\"target_label\"])\n",
    "df_val[\"target_label\"]=L.transform(df_val[\"target_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.reset_index(drop=True)\n",
    "df_val=df_val.reset_index(drop=True)\n",
    "df_test=df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"text_data\"]=df_train[\"text_data\"].astype(str)\n",
    "df_val[\"text_data\"]=df_val[\"text_data\"].astype(str)\n",
    "df_test[\"text_data\"]=df_test[\"text_data\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分類するクラス数\n",
    "num_class=df_train[\"target_label\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_text_train=df_train.tweet.values\n",
    "distance_list_train=df_train.code.values\n",
    "dataset_for_loader_train = []\n",
    "i=0\n",
    "for sentence in sentences_text_train:\n",
    "    encoding = tokenizer(\n",
    "                sentence,                      \n",
    "                max_length = 100,           # 文章の長さを固定（Padding/Trancatinating）\n",
    "                pad_to_max_length = True,# PADDINGで埋める\n",
    "                truncation=True,\n",
    "                #return_tensors = 'pt'\n",
    "                )\n",
    "    encoding['labels'] = distance_list_train[i]\n",
    "    i=i+1\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader_train.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_text_val=df_val.tweet.values\n",
    "distance_list_val=df_val.code.values\n",
    "dataset_for_loader_val = []\n",
    "i=0\n",
    "for sentence in sentences_text_val:\n",
    "    encoding = tokenizer(\n",
    "                sentence,                      \n",
    "                max_length = 100,           # 文章の長さを固定（Padding/Trancatinating）\n",
    "                pad_to_max_length = True,# PADDINGで埋める\n",
    "                truncation=True,\n",
    "                #return_tensors = 'pt'\n",
    "                )\n",
    "    encoding['labels'] = distance_list_val[i]\n",
    "    i=i+1\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader_val.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_text_test=df_test.tweet.values\n",
    "distance_list_test=df_test.code.values\n",
    "dataset_for_loader_test = []\n",
    "i=0\n",
    "for sentence in sentences_text_test:\n",
    "    encoding = tokenizer(\n",
    "                sentence,                      \n",
    "                max_length = 107,           # 文章の長さを固定（Padding/Trancatinating）\n",
    "                pad_to_max_length = True,# PADDINGで埋める\n",
    "                truncation=True,\n",
    "                #return_tensors = 'pt'\n",
    "                )\n",
    "    encoding['labels'] = distance_list_test[i]\n",
    "    i=i+1\n",
    "    encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n",
    "    dataset_for_loader_test.append(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_for_loader_train\n",
    "dataset_val = dataset_for_loader_val\n",
    "dataset_test = dataset_for_loader_test\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=batch) \n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from transformers import BertModel\n",
    "class BertForSequenceClassifier_pl(pl.LightningModule):\n",
    "    def __init__(self, model_name, lr, num_class):\n",
    "        # model_name: Transformersのモデルの名前\n",
    "        # num_labels: ラベルの数\n",
    "        # lr: 学習率\n",
    "        super().__init__()\n",
    "        # 引数のnum_labelsとlrを保存。\n",
    "        # 例えば、self.hparams.lrでlrにアクセスできる。\n",
    "        # チェックポイント作成時にも自動で保存される。\n",
    "        self.save_hyperparameters()\n",
    "        # BERTのロード\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_class)\n",
    "        #損失関数の設定回帰であればnn.MSELossなどに\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        # BertLayerモジュールの最後を勾配計算ありに変更\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.bert.encoder.layer[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        preds= self.classifier(output.pooler_output)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(preds, labels)\n",
    "        #print(f\"tihi is {loss}\")\n",
    "        return loss, preds\n",
    "    # trainのミニバッチに対して行う処理\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, preds = self.forward(input_ids=batch[\"input_ids\"],\n",
    "                                    attention_mask=batch[\"attention_mask\"],\n",
    "                                    labels=batch[\"labels\"])\n",
    "        self.log('train_loss', loss)\n",
    "        return {'loss': loss,\n",
    "                'batch_preds': preds,\n",
    "                'batch_labels': batch[\"labels\"]}\n",
    "\n",
    "    # validation、testでもtrain_stepと同じ処理を行う\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, preds = self.forward(input_ids=batch[\"input_ids\"],\n",
    "                                    attention_mask=batch[\"attention_mask\"],\n",
    "                                    labels=batch[\"labels\"])\n",
    "        return {'loss': loss,\n",
    "                'batch_preds': preds,\n",
    "                'batch_labels': batch[\"labels\"]}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, preds = self.forward(input_ids=batch[\"input_ids\"],\n",
    "                                    attention_mask=batch[\"attention_mask\"],\n",
    "                                    labels=batch[\"labels\"])\n",
    "        return {'loss': loss,\n",
    "                'batch_preds': preds,\n",
    "                'batch_labels': batch[\"labels\"]}\n",
    "\n",
    "    # epoch終了時にvalidationのlossとaccuracyを記録\n",
    "    def validation_epoch_end(self, outputs, mode=\"val\"):\n",
    "        # loss計算\n",
    "        epoch_preds = torch.cat([x['batch_preds'] for x in outputs])\n",
    "        epoch_labels = torch.cat([x['batch_labels'] for x in outputs])\n",
    "        epoch_loss = self.criterion(epoch_preds, epoch_labels)\n",
    "        self.log(f\"{mode}_loss\", epoch_loss, logger=True)\n",
    "\n",
    "        num_correct = (epoch_preds.argmax(dim=1) == epoch_labels).sum().item()\n",
    "        epoch_accuracy = num_correct / len(epoch_labels)\n",
    "        self.log(f\"{mode}_accuracy\", epoch_accuracy, logger=True)\n",
    "\n",
    "    # testデータのlossとaccuracyを算出（validationの使いまわし）\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.validation_epoch_end(outputs, \"test\")\n",
    "\n",
    "    # 学習に用いるオプティマイザを返す関数を書く。\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_top_k=1,\n",
    "    save_weights_only=True,\n",
    "    dirpath='model/',#モデルを保存する先を指定\n",
    ")\n",
    "\n",
    "# 学習の方法を指定\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=5,#epoch数の指定\n",
    "    callbacks=[checkpoint]\n",
    ")\n",
    "model = BertForSequenceClassifier_pl(\n",
    "    model_name=MODEL_NAME, lr=1e-5 , num_class=num_class+1\n",
    ")\n",
    "\n",
    "# ファインチューニングを行う。\n",
    "trainer.fit(model, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = trainer.test(dataloaders=dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = checkpoint.best_model_path # ベストモデルのファイル\n",
    "print(best_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('SharedTask')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6feaf4af813783365efc585a6800850988678764fcb0bba12eca45fe4d31fdb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
